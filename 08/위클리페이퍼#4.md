# 📘 [AI] 위클리페이퍼 #4  
---
> **작성일**: 2025년 8월 3일  
> **작성자**: 이유노  

## 🔹 01. 딥러닝과 머신러닝 간의 포함 관계는 무엇인가요?  

### 📌 딥러닝과 머신러닝의 관계  
- **머신러닝(ML)** 은 데이터를 기반으로 모델이 규칙을 학습하도록 하는 인공지능의 한 분야입니다.  
- **딥러닝(DL)** 은 머신러닝의 한 방법으로, **인공신경망(Artificial Neural Network)** 을 기반으로 한 알고리즘을 사용해 데이터를 학습합니다.  

📌 **포함 관계**  
> **인공지능(AI)** ⊃ **머신러닝(ML)** ⊃ **딥러닝(DL)**  

즉, 딥러닝은 머신러닝의 하위 개념이며, 여러 층의 인공신경망(다층 퍼셉트론, CNN, RNN 등)을 활용해 복잡한 데이터 패턴을 학습하는 방법입니다.  

---

### :thumbsup: 딥러닝의 특징  
1. **자동 특징 추출** – 사람이 직접 특징을 설계하지 않아도 데이터에서 중요한 특징을 스스로 학습합니다.  
2. **대규모 데이터와 연산 자원 활용** – 데이터와 연산량이 많을수록 성능이 향상됩니다.  
3. **비선형 문제 처리 가능** – 활성화 함수를 통해 복잡한 비선형 관계를 학습할 수 있습니다.  

---

### :thumbsdown: 한계  
1. **많은 데이터와 연산 자원이 필요**  
2. **모델 해석이 어려움 (Black-box 특성)**  
3. **하이퍼파라미터 설정에 따라 성능 차이가 큼**  

---

## 🔹 02. 딥러닝 성능 향상을 위해 고려하는 하이퍼파라미터의 종류는 무엇인가요?  

### 📌 하이퍼파라미터란?  
- 모델 학습 전에 사용자가 직접 설정하는 값으로, 모델의 구조나 학습 방법을 결정합니다.  
- **Weight, Bias**와 달리 학습으로 자동 조정되지 않고, 사용자가 직접 값을 정해야 합니다.  

---

### 📌 주요 하이퍼파라미터 종류  

| 구분 | 하이퍼파라미터 | 설명 |
|------|---------------|------|
| **모델 구조** | 은닉층 개수, 뉴런 수 | 층의 수와 각 층의 노드 개수를 조절하여 모델의 복잡도를 설정 |
| **학습률** | Learning Rate | 가중치 업데이트 크기를 결정. 너무 크면 발산, 너무 작으면 학습이 느려짐 |
| **배치 크기** | Batch Size | 한 번 학습에서 사용하는 데이터 샘플 개수. 메모리 사용량과 학습 안정성에 영향 |
| **에폭 수** | Epoch | 전체 데이터를 몇 번 반복 학습할지 결정 |
| **가중치 초기화** | Xavier, He 초기화 | 초기값 설정 방식으로 기울기 소실/폭발 문제를 방지 |
| **정규화** | Dropout 비율 | 과적합을 방지하기 위해 뉴런을 일정 확률로 무시 |
| **최적화 알고리즘** | SGD, Adam, RMSProp 등 | 가중치를 업데이트하는 방법을 결정 |
| **활성화 함수** | ReLU, Sigmoid, Tanh 등 | 비선형성을 추가하여 복잡한 패턴 학습 가능 |

---

### :thumbsup: 적절한 하이퍼파라미터 선택의 장점  
- 학습 속도와 성능을 동시에 개선  
- 과적합과 기울기 소실 문제를 완화  

### :thumbsdown: 잘못된 설정 시 문제  
- 학습이 너무 느리거나,  
- 손실이 발산하여 학습이 실패할 수 있음  

---

> **정리**  
> 머신러닝과 딥러닝은 포함 관계에 있으며, 딥러닝은 인공신경망을 활용한 머신러닝 기법입니다.  
> 성능 향상을 위해 학습률, 초기화 방식, 최적화 알고리즘, 정규화 기법 등 다양한 하이퍼파라미터를 조정해야 합니다.
