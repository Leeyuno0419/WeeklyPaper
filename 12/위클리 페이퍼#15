## 📘 [AI] 위클리페이퍼 #15

> **작성일**: 2025년 12월 15일
> **작성자**: 이유노

---

## 🔹 01. 딥러닝 모델을 ONNX, TensorRT 등의 포맷으로 변환해야 하는 이유는 무엇인가요?

딥러닝 모델은 보통 **PyTorch, TensorFlow** 같은 프레임워크에서 학습됩니다.
하지만 **실제 서비스 환경**에서는 학습 프레임워크 그대로 사용하는 것이
성능·배포·운영 측면에서 비효율적인 경우가 많습니다.

이때 등장하는 것이 **ONNX, TensorRT와 같은 추론 최적화 포맷**입니다.

---

### 📌 ONNX(Open Neural Network Exchange)란?

> **딥러닝 모델을 프레임워크 독립적으로 표현하기 위한 표준 포맷**

ONNX의 핵심 목적은 다음과 같습니다.

* PyTorch → TensorFlow → TensorRT 등 **프레임워크 간 호환**
* 학습과 추론 환경 분리
* 다양한 하드웨어 및 추론 엔진에서 재사용 가능

즉, ONNX는
**“학습용 모델을 서비스용 중간 포맷으로 변환하는 표준 인터페이스”** 역할을 합니다.

---

### 📌 TensorRT란?

> **NVIDIA GPU 환경에 특화된 고성능 추론 엔진**

TensorRT는 단순 포맷이 아니라,

* 연산 그래프 최적화
* 레이어 퓨전(Layer Fusion)
* FP16 / INT8 양자화
* GPU 메모리 최적화

등을 통해 **실제 추론 속도를 극대화**합니다.

---

### 📌 왜 변환이 필요한가?

| 관점  | 변환이 필요한 이유                   |
| --- | ---------------------------- |
| 성능  | 추론 속도 감소, 지연 시간(Latency) 최소화 |
| 배포  | 학습 프레임워크 의존성 제거              |
| 호환성 | 다양한 런타임·하드웨어에서 실행            |
| 운영  | 서버 비용 절감 및 안정성 확보            |

---

### ✅ 핵심 정리

> **학습 프레임워크는 ‘모델을 잘 만드는 도구’이고,
> ONNX·TensorRT는 ‘모델을 빠르고 안정적으로 서비스하기 위한 도구’입니다.**

---

## 🔹 02. 양자화 기법인 Post-Training Quantization과 Quantization-Aware Training의 차이를 설명해보세요.

양자화(Quantization)는
모델의 **연산 정밀도를 낮춰** 성능과 효율을 개선하는 대표적인 경량화 기법입니다.

대표적인 두 가지 방식이 있습니다.

---

### 📌 Post-Training Quantization (PTQ)

> **학습이 끝난 모델에 사후적으로 양자화를 적용하는 방식**

**특징**

* 추가 학습 없이 바로 적용 가능
* 구현이 간단하고 빠름
* 소량의 Calibration 데이터만 필요

**단점**

* 정밀도 감소로 인한 정확도 하락 가능성
* 복잡한 모델일수록 성능 저하 위험 증가

**적합한 상황**

* 빠른 실험
* 정확도 여유가 있는 모델
* MVP 또는 초기 서비스 단계

---

### 📌 Quantization-Aware Training (QAT)

> **학습 과정에서부터 양자화를 고려하여 모델을 학습하는 방식**

**특징**

* 학습 중 양자화 오차를 미리 반영
* PTQ 대비 정확도 손실 최소화
* 실제 INT8 추론 환경과 가장 유사

**단점**

* 재학습 필요
* 학습 시간 및 구현 복잡도 증가

**적합한 상황**

* 정확도가 매우 중요한 서비스
* 대규모 트래픽을 처리하는 상용 시스템

---

### 📌 PTQ vs QAT 비교

| 구분       | PTQ      | QAT  |
| -------- | -------- | ---- |
| 적용 시점    | 학습 후     | 학습 중 |
| 추가 학습    | 필요 없음    | 필요   |
| 구현 난이도   | 낮음       | 높음   |
| 정확도 유지   | 상대적으로 낮음 | 높음   |
| 실서비스 적합성 | 중        | 높음   |

---

### ✅ 핵심 정리

> **PTQ는 ‘빠르고 간단한 양자화’,
> QAT는 ‘정확도를 지키는 양자화’라고 이해할 수 있습니다.**

---

## 🔹 03. 양자화나 모델 경량화 후 실제 서비스에서 성능이 저하되지 않도록 하기 위해 어떤 테스트나 확인 절차가 필요할까요?

모델 경량화는 **속도 향상**이라는 장점이 있지만,
잘못 적용하면 **정확도·안정성·신뢰성**이 함께 무너질 수 있습니다.

따라서 서비스 전 반드시 다음과 같은 검증 절차가 필요합니다.

---

### 📌 ① 정확도 비교 테스트

* 원본 모델 vs 경량화 모델
* 동일한 검증 데이터셋 사용
* Accuracy, Precision, Recall, F1 등 비교

👉 **허용 가능한 성능 저하 범위 정의가 핵심**

---

### 📌 ② 실제 추론 환경 성능 측정

* Latency (평균 / P95 / P99)
* Throughput (QPS)
* GPU/CPU 메모리 사용량

👉 **로컬 테스트가 아닌 실제 배포 환경 기준**으로 측정해야 의미가 있습니다.

---

### 📌 ③ 대표 입력 및 엣지 케이스 테스트

* 극단값, 경계값 입력
* 서비스에서 자주 발생하는 입력 패턴
* 오류 발생 시 fallback 동작 확인

👉 정확도 평균보다 **“망가질 수 있는 케이스”** 를 찾는 것이 중요합니다.

---

### 📌 ④ A/B 테스트 또는 Shadow Deployment

* 일부 트래픽만 경량화 모델에 적용
* 기존 모델과 결과 비교
* 사용자 영향 최소화

👉 대규모 서비스일수록 필수적인 단계입니다.

---

### 📌 ⑤ 모니터링 및 롤백 전략

* 추론 결과 분포 변화 감지
* 성능 저하 시 즉시 이전 모델로 롤백 가능 구조

---

### ✅ 핵심 정리

> **모델 경량화의 성공 기준은
> “빠른 모델”이 아니라
> “빠르면서도 신뢰 가능한 모델”입니다.**

---

## ✅ 최종 요약

* **ONNX / TensorRT 변환**은 학습 모델을 실서비스에 적합하게 만들기 위한 필수 단계입니다.
* **PTQ는 간편함**, **QAT는 정확도 유지**에 강점이 있습니다.
* 경량화 이후에는 **정확도·성능·안정성**을 모두 검증하는 테스트 절차가 반드시 필요합니다.
