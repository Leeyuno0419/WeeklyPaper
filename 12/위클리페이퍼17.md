## 🎄 📘 [AI] 위클리페이퍼 #17

> **작성일**: 2025년 12월 29일
> **작성자**: 이유노

---

## 🔹 01. 모델 서빙이란 무엇이며, 왜 필요한가요? 실제 서비스 환경에서 서빙 프레임워크는 어떤 역할을 하나요?

딥러닝 모델은 학습이 끝났다고 해서 곧바로 서비스에 사용할 수 있는 상태는 아닙니다.
**사용자 요청에 따라 안정적이고 빠르게 추론 결과를 제공하는 과정**,
이것이 바로 **모델 서빙(Model Serving)** 입니다.

---

### 📌 모델 서빙(Model Serving)이란?

> **학습된 모델을 서비스 환경에서 API 형태로 제공하여
> 실시간 또는 배치 추론이 가능하도록 만드는 과정**

모델 서빙은 단순히 `predict()`를 호출하는 것이 아니라, 다음을 모두 포함합니다.

* 외부 요청 수신 (HTTP / gRPC)
* 입력 데이터 전처리
* 모델 추론 실행
* 결과 후처리 및 응답 반환
* 성능 모니터링 및 장애 대응

---

### 📌 왜 모델 서빙이 필요한가?

학습 환경과 서비스 환경은 본질적으로 다릅니다.

| 구분    | 학습 환경   | 서비스 환경    |
| ----- | ------- | --------- |
| 목적    | 정확도 최대화 | 응답 속도·안정성 |
| 요청    | 배치 중심   | 실시간 요청    |
| 실패 허용 | 비교적 높음  | 매우 낮음     |
| 확장성   | 중요하지 않음 | 필수        |

👉 단순 스크립트 형태의 모델은
**동시 요청 처리, 장애 복구, 확장성**을 보장할 수 없습니다.

---

### 📌 서빙 프레임워크의 역할

> **모델을 ‘운영 가능한 서비스’로 바꿔주는 인프라 계층**

서빙 프레임워크는 다음 역할을 담당합니다.

* 고성능 추론 엔진 연동 (ONNX, TensorRT 등)
* 동시 요청 처리 및 배치 최적화
* GPU 자원 효율적 사용
* 모델 버전 관리 및 핫스왑
* 헬스 체크 및 메트릭 수집

---

### ✅ 핵심 정리

> **모델 서빙은 ‘모델을 잘 만드는 것’과
> ‘모델을 잘 쓰는 것’ 사이를 잇는 핵심 단계입니다.**

---

## 🔹 02. Streamlit, FastAPI로 구성된 시스템에 Triton 기반 추론 서버를 통합하려면 어떤 구조로 설계하는 것이 바람직하다고 생각하나요?

Streamlit + FastAPI + Triton 조합은
**프론트–백엔드–추론 서버의 역할을 명확히 분리**하는 구조가 핵심입니다.

---

### 📌 각 컴포넌트의 역할 분리

| 구성 요소                   | 역할             |
| ----------------------- | -------------- |
| Streamlit               | 사용자 UI, 결과 시각화 |
| FastAPI                 | 비즈니스 로직, 요청 제어 |
| Triton Inference Server | 고성능 모델 추론      |

👉 **UI / API / 추론을 분리**함으로써 각 계층을 독립적으로 확장·교체할 수 있습니다.

---

### 📌 권장 아키텍처 구조

```text
[User]
  ↓
[Streamlit]
  ↓ HTTP
[FastAPI]
  ↓ gRPC / HTTP
[Triton Inference Server]
  ↓
[Model (ONNX / TensorRT)]
```

---

### 📌 FastAPI를 중간 계층으로 두는 이유

FastAPI는 단순 전달자가 아니라 **컨트롤 타워 역할**을 합니다.

* 입력 검증 및 전처리
* 요청 큐잉 및 Rate Limiting
* 여러 모델 라우팅
* 결과 후처리
* 인증·로깅·모니터링

👉 Streamlit이 직접 Triton을 호출하지 않고
**FastAPI를 통해 간접 접근**하는 것이 유지보수와 보안 측면에서 유리합니다.

---

### 📌 Triton을 추론 서버로 사용하는 이유

> **“모델 서빙에 특화된 전문 서버”**

Triton은 다음 강점을 가집니다.

* 다중 모델 동시 서빙
* 동적 배치(Dynamic Batching)
* GPU 최적화
* 모델 핫 리로드
* 프레임워크 독립적 추론

👉 FastAPI에서 직접 모델을 로딩하는 방식보다
**성능·확장성·운영 안정성**에서 압도적으로 유리합니다.

---

### 📌 확장 고려 사항

* 모델 버전별 엔드포인트 분리
* A/B 테스트용 모델 병렬 서빙
* Docker / Kubernetes 기반 배포
* GPU 리소스 분리 및 할당 전략

---

### ✅ 핵심 정리

> **Streamlit은 ‘보여주는 역할’,
> FastAPI는 ‘판단하는 역할’,
> Triton은 ‘계산하는 역할’을 맡는 구조가 이상적입니다.**

---

## ✅ 최종 요약

* **모델 서빙**은 학습된 모델을 실제 사용자 요청에 안정적으로 제공하기 위한 필수 과정입니다.
* 서빙 프레임워크는 성능, 확장성, 운영 안정성을 담당하는 핵심 인프라입니다.
* **Streamlit–FastAPI–Triton** 구조는 역할 분리가 명확하고 실서비스에 적합한 아키텍처입니다.
