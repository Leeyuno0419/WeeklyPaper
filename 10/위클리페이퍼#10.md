# 📘 [AI] 위클리페이퍼 #10

---

> **작성일**: 2025년 10월 13일
>
> **작성자**: 이유노

---

## 🔹 01. 텍스트 데이터를 모델에 적용하기 전에 어떤 전처리 과정을 거치나요?

### 📌 텍스트 전처리(Text Preprocessing)의 목적

자연어는 사람이 이해하기엔 직관적이지만, 모델이 처리하기엔 **불규칙적이고 비정형적인 데이터**입니다. 따라서 텍스트를 모델이 이해할 수 있는 수치 형태로 바꾸기 위해 여러 단계의 전처리가 필요합니다.

| 단계                                              | 설명                                      | 예시                                        |
| :---------------------------------------------- | :-------------------------------------- | :---------------------------------------- |
| **1️⃣ 정제(Cleaning)**                            | 불필요한 문자, HTML 태그, 특수기호, 중복 공백 등을 제거     | “안녕하세요!!! 😀” → “안녕하세요”                   |
| **2️⃣ 토큰화(Tokenization)**                       | 문장을 단어, 형태소, 혹은 서브워드 단위로 분리             | “나는 학교에 갔다” → [“나”, “는”, “학교”, “에”, “갔다”] |
| **3️⃣ 불용어 제거(Stopword Removal)**                | 의미가 거의 없는 단어(예: ‘은’, ‘는’, ‘이’, ‘가’) 제거  | “나는 학교에 갔다” → [“학교”, “갔다”]                |
| **4️⃣ 어간 추출(Stemming) / 표제어 추출(Lemmatization)** | 단어의 기본형으로 변환                            | “갔다”, “가고”, “가면” → “가다”                   |
| **5️⃣ 인코딩(Encoding)**                           | 단어를 수치형 데이터로 변환 (예: 정수 인덱스, 원-핫, 임베딩 등) | “학교” → [0.1, 0.3, -0.2, …]                |
| **6️⃣ 패딩(Padding)**                             | 문장 길이를 일정하게 맞춤 (모델 입력 크기 통일 목적)         | [12, 33, 24] → [12, 33, 24, 0, 0]         |

> **정리** 💡
> 텍스트 전처리는
> 1️⃣ **불필요한 요소 제거** →
> 2️⃣ **단어 단위 분리** →
> 3️⃣ **수치형 표현으로 변환**
> 의 3단계 구조로 볼 수 있습니다.

---

## 🔹 02. FastText가 Word2Vec과 다른 점은 무엇이며, 어떤 장점이 있나요?

### 📌 Word2Vec의 한계

Word2Vec은 각 단어를 하나의 벡터로 표현하는 모델로, **단어 단위(Word-level)** 학습만 수행합니다.
이로 인해 다음과 같은 한계가 있습니다:

* **희귀 단어(OOV, Out-Of-Vocabulary)** 문제 발생
* **형태가 유사한 단어 간 의미 공유 부족** (예: “run”, “running”, “runner”)

### 📌 FastText의 핵심 아이디어

**FastText (Facebook AI, 2016)**는 단어를 **여러 개의 문자 단위 N-gram(부분 단어)**로 쪼개서 학습합니다.
예를 들어, “apple”을 3-gram으로 분해하면:

> `<ap`, `app`, `ppl`, `ple`, `le>`

각 N-gram 벡터를 평균 내어 단어 벡터를 구성하므로, **형태적으로 유사한 단어들이 비슷한 벡터 공간에 위치**하게 됩니다.

| 비교 항목         | **Word2Vec**    | **FastText**                |
| :------------ | :-------------- | :-------------------------- |
| **학습 단위**     | 단어 (Word-level) | 서브워드(Subword-level, N-gram) |
| **OOV 대응**    | 불가능             | 부분 단어 단위로 벡터 생성 가능          |
| **형태소 정보 반영** | 없음              | 있음 (접두사·접미사 반영)             |
| **학습 속도**     | 빠름              | 약간 느림 (추가 연산 필요)            |

> **장점 요약** 🌟
>
> * 새로운 단어(OOV)에 대한 **일반화 성능 향상**
> * **언어의 형태적 특성**(예: 어미, 접두사, 접미사)을 반영
> * **소량 데이터 환경에서도 안정적**

---

## 🔹 03. Attention 메커니즘이 Seq2Seq 모델의 어떤 문제를 해결하는 데 도움이 되나요?

### 📌 Seq2Seq 모델의 구조와 한계

기본적인 **Sequence-to-Sequence(Seq2Seq)** 모델은

* **인코더(Encoder)**: 입력 시퀀스를 하나의 고정 벡터로 인코딩
* **디코더(Decoder)**: 해당 벡터를 기반으로 출력 시퀀스를 생성
  하는 구조입니다.

하지만 이 방식에는 근본적인 문제가 있습니다:

> 긴 문장의 정보를 **하나의 고정 벡터**에 압축하므로
> **정보 손실**과 **문맥 흐름 단절**이 발생합니다.

### 📌 Attention 메커니즘의 핵심 아이디어

**Attention**은 “모든 단어를 동일하게 다루지 말고, **현재 출력 단어를 생성할 때 중요한 입력 단어에 더 집중하자**”는 개념입니다.

즉, 디코더가 매 시점마다
입력 문장의 모든 단어(hidden state)에 가중치(attention score)를 부여하여,
**가장 관련 있는 정보에 ‘집중(attend)’**하게 만듭니다.

| 항목        | 설명                                   |
| :-------- | :----------------------------------- |
| **핵심 개념** | 디코더가 입력 시퀀스의 각 단어에 중요도(가중치)를 동적으로 계산 |
| **효과**    | 문맥이 긴 문장에서도 관련 단어 정보를 선택적으로 반영       |
| **결과**    | 더 자연스럽고 문맥 일관성 있는 번역/생성 가능           |

> **비유** 🎯
> 사람도 문장을 이해할 때 모든 단어를 동일하게 보지 않고,
> “핵심 단어”에 집중합니다.
> Attention은 이런 **인간의 인지 방식**을 수학적으로 모델링한 것입니다.

---

## 🔹 04. Transformer 모델은 Seq2Seq 구조와 어떤 점에서 근본적으로 다른가요?

### 📌 기존 Seq2Seq + Attention의 한계

RNN 기반 Seq2Seq 모델은 Attention을 도입해도 여전히 **순차적(Sequential)** 구조를 따릅니다.
즉, 입력을 순서대로 처리해야 하므로 **병렬 처리 불가**, **긴 문장 학습 한계**, **시간 소모** 문제가 존재했습니다.

### 📌 Transformer의 혁신적 변화

**Transformer (Vaswani et al., 2017)**는 RNN 구조를 완전히 제거하고,
**Attention만으로 시퀀스를 처리하는 모델**입니다.
이를 통해 **병렬 연산**, **장기 의존성(Long Dependency) 처리**, **속도 향상**이 동시에 가능해졌습니다.

| 비교 항목                           | **기존 Seq2Seq (RNN 기반)** | **Transformer**                           |
| :------------------------------ | :---------------------- | :---------------------------------------- |
| **기반 구조**                       | RNN/LSTM/GRU (순차 처리)    | Self-Attention (병렬 처리 가능)                 |
| **문장 내 의존성 처리**                 | 단기 의존성 중심               | 장기 의존성까지 효율적으로 학습                         |
| **병렬 연산**                       | 불가능                     | 가능 (GPU 효율적 활용)                           |
| **대표 구성 요소**                    | 인코더 + 디코더 (RNN 구조)      | 인코더 + 디코더 (Self-Attention + Feed Forward) |
| **포지셔널 인코딩(Position Encoding)** | 필요 없음 (순서 내장)           | 필수 (순서 정보를 보완)                            |

> **핵심 요약** ⚙️
>
> * Transformer는 **“Attention is All You Need”**라는 철학 아래
>   RNN을 완전히 대체했습니다.
> * 덕분에 현대의 대규모 언어 모델(GPT, BERT 등)의 기반이 되었습니다.

---

> **정리 요약** 🧩
>
> * **텍스트 전처리**: 텍스트를 정제 → 토큰화 → 수치화 → 패딩
> * **FastText**: Word2Vec의 한계를 극복한 **서브워드 기반 임베딩**
> * **Attention**: Seq2Seq의 **정보 압축 문제 해결**, 문맥 선택 가능
> * **Transformer**: RNN 제거 + **Self-Attention 기반 병렬 구조**로 혁신

---

**출처 및 참고**

* Vaswani et al., *Attention is All You Need* (2017)
* Mikolov et al., *Word2Vec: Efficient Estimation of Word Representations in Vector Space* (2013)
* Bojanowski et al., *Enriching Word Vectors with Subword Information (FastText)* (2016)
* 김기현, 《딥러닝 자연어처리 완벽 가이드》 (2023)
