# ğŸ“˜ [AI] ìœ„í´ë¦¬í˜ì´í¼ #10

---

> **ì‘ì„±ì¼**: 2025ë…„ 10ì›” 13ì¼
>
> **ì‘ì„±ì**: ì´ìœ ë…¸

---

## ğŸ”¹ 01. í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ëª¨ë¸ì— ì ìš©í•˜ê¸° ì „ì— ì–´ë–¤ ì „ì²˜ë¦¬ ê³¼ì •ì„ ê±°ì¹˜ë‚˜ìš”?

### ğŸ“Œ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬(Text Preprocessing)ì˜ ëª©ì 

ìì—°ì–´ëŠ” ì‚¬ëŒì´ ì´í•´í•˜ê¸°ì—” ì§ê´€ì ì´ì§€ë§Œ, ëª¨ë¸ì´ ì²˜ë¦¬í•˜ê¸°ì—” **ë¶ˆê·œì¹™ì ì´ê³  ë¹„ì •í˜•ì ì¸ ë°ì´í„°**ì…ë‹ˆë‹¤. ë”°ë¼ì„œ í…ìŠ¤íŠ¸ë¥¼ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ìˆ˜ì¹˜ í˜•íƒœë¡œ ë°”ê¾¸ê¸° ìœ„í•´ ì—¬ëŸ¬ ë‹¨ê³„ì˜ ì „ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.

| ë‹¨ê³„                                              | ì„¤ëª…                                      | ì˜ˆì‹œ                                        |
| :---------------------------------------------- | :-------------------------------------- | :---------------------------------------- |
| **1ï¸âƒ£ ì •ì œ(Cleaning)**                            | ë¶ˆí•„ìš”í•œ ë¬¸ì, HTML íƒœê·¸, íŠ¹ìˆ˜ê¸°í˜¸, ì¤‘ë³µ ê³µë°± ë“±ì„ ì œê±°     | â€œì•ˆë…•í•˜ì„¸ìš”!!! ğŸ˜€â€ â†’ â€œì•ˆë…•í•˜ì„¸ìš”â€                   |
| **2ï¸âƒ£ í† í°í™”(Tokenization)**                       | ë¬¸ì¥ì„ ë‹¨ì–´, í˜•íƒœì†Œ, í˜¹ì€ ì„œë¸Œì›Œë“œ ë‹¨ìœ„ë¡œ ë¶„ë¦¬             | â€œë‚˜ëŠ” í•™êµì— ê°”ë‹¤â€ â†’ [â€œë‚˜â€, â€œëŠ”â€, â€œí•™êµâ€, â€œì—â€, â€œê°”ë‹¤â€] |
| **3ï¸âƒ£ ë¶ˆìš©ì–´ ì œê±°(Stopword Removal)**                | ì˜ë¯¸ê°€ ê±°ì˜ ì—†ëŠ” ë‹¨ì–´(ì˜ˆ: â€˜ì€â€™, â€˜ëŠ”â€™, â€˜ì´â€™, â€˜ê°€â€™) ì œê±°  | â€œë‚˜ëŠ” í•™êµì— ê°”ë‹¤â€ â†’ [â€œí•™êµâ€, â€œê°”ë‹¤â€]                |
| **4ï¸âƒ£ ì–´ê°„ ì¶”ì¶œ(Stemming) / í‘œì œì–´ ì¶”ì¶œ(Lemmatization)** | ë‹¨ì–´ì˜ ê¸°ë³¸í˜•ìœ¼ë¡œ ë³€í™˜                            | â€œê°”ë‹¤â€, â€œê°€ê³ â€, â€œê°€ë©´â€ â†’ â€œê°€ë‹¤â€                   |
| **5ï¸âƒ£ ì¸ì½”ë”©(Encoding)**                           | ë‹¨ì–´ë¥¼ ìˆ˜ì¹˜í˜• ë°ì´í„°ë¡œ ë³€í™˜ (ì˜ˆ: ì •ìˆ˜ ì¸ë±ìŠ¤, ì›-í•«, ì„ë² ë”© ë“±) | â€œí•™êµâ€ â†’ [0.1, 0.3, -0.2, â€¦]                |
| **6ï¸âƒ£ íŒ¨ë”©(Padding)**                             | ë¬¸ì¥ ê¸¸ì´ë¥¼ ì¼ì •í•˜ê²Œ ë§ì¶¤ (ëª¨ë¸ ì…ë ¥ í¬ê¸° í†µì¼ ëª©ì )         | [12, 33, 24] â†’ [12, 33, 24, 0, 0]         |

> **ì •ë¦¬** ğŸ’¡
> í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ëŠ”
> 1ï¸âƒ£ **ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°** â†’
> 2ï¸âƒ£ **ë‹¨ì–´ ë‹¨ìœ„ ë¶„ë¦¬** â†’
> 3ï¸âƒ£ **ìˆ˜ì¹˜í˜• í‘œí˜„ìœ¼ë¡œ ë³€í™˜**
> ì˜ 3ë‹¨ê³„ êµ¬ì¡°ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## ğŸ”¹ 02. FastTextê°€ Word2Vecê³¼ ë‹¤ë¥¸ ì ì€ ë¬´ì—‡ì´ë©°, ì–´ë–¤ ì¥ì ì´ ìˆë‚˜ìš”?

### ğŸ“Œ Word2Vecì˜ í•œê³„

Word2Vecì€ ê° ë‹¨ì–´ë¥¼ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ í‘œí˜„í•˜ëŠ” ëª¨ë¸ë¡œ, **ë‹¨ì–´ ë‹¨ìœ„(Word-level)** í•™ìŠµë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤.
ì´ë¡œ ì¸í•´ ë‹¤ìŒê³¼ ê°™ì€ í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤:

* **í¬ê·€ ë‹¨ì–´(OOV, Out-Of-Vocabulary)** ë¬¸ì œ ë°œìƒ
* **í˜•íƒœê°€ ìœ ì‚¬í•œ ë‹¨ì–´ ê°„ ì˜ë¯¸ ê³µìœ  ë¶€ì¡±** (ì˜ˆ: â€œrunâ€, â€œrunningâ€, â€œrunnerâ€)

### ğŸ“Œ FastTextì˜ í•µì‹¬ ì•„ì´ë””ì–´

**FastText (Facebook AI, 2016)**ëŠ” ë‹¨ì–´ë¥¼ **ì—¬ëŸ¬ ê°œì˜ ë¬¸ì ë‹¨ìœ„ N-gram(ë¶€ë¶„ ë‹¨ì–´)**ë¡œ ìª¼ê°œì„œ í•™ìŠµí•©ë‹ˆë‹¤.
ì˜ˆë¥¼ ë“¤ì–´, â€œappleâ€ì„ 3-gramìœ¼ë¡œ ë¶„í•´í•˜ë©´:

> `<ap`, `app`, `ppl`, `ple`, `le>`

ê° N-gram ë²¡í„°ë¥¼ í‰ê·  ë‚´ì–´ ë‹¨ì–´ ë²¡í„°ë¥¼ êµ¬ì„±í•˜ë¯€ë¡œ, **í˜•íƒœì ìœ¼ë¡œ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ì´ ë¹„ìŠ·í•œ ë²¡í„° ê³µê°„ì— ìœ„ì¹˜**í•˜ê²Œ ë©ë‹ˆë‹¤.

| ë¹„êµ í•­ëª©         | **Word2Vec**    | **FastText**                |
| :------------ | :-------------- | :-------------------------- |
| **í•™ìŠµ ë‹¨ìœ„**     | ë‹¨ì–´ (Word-level) | ì„œë¸Œì›Œë“œ(Subword-level, N-gram) |
| **OOV ëŒ€ì‘**    | ë¶ˆê°€ëŠ¥             | ë¶€ë¶„ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë²¡í„° ìƒì„± ê°€ëŠ¥          |
| **í˜•íƒœì†Œ ì •ë³´ ë°˜ì˜** | ì—†ìŒ              | ìˆìŒ (ì ‘ë‘ì‚¬Â·ì ‘ë¯¸ì‚¬ ë°˜ì˜)             |
| **í•™ìŠµ ì†ë„**     | ë¹ ë¦„              | ì•½ê°„ ëŠë¦¼ (ì¶”ê°€ ì—°ì‚° í•„ìš”)            |

> **ì¥ì  ìš”ì•½** ğŸŒŸ
>
> * ìƒˆë¡œìš´ ë‹¨ì–´(OOV)ì— ëŒ€í•œ **ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ**
> * **ì–¸ì–´ì˜ í˜•íƒœì  íŠ¹ì„±**(ì˜ˆ: ì–´ë¯¸, ì ‘ë‘ì‚¬, ì ‘ë¯¸ì‚¬)ì„ ë°˜ì˜
> * **ì†ŒëŸ‰ ë°ì´í„° í™˜ê²½ì—ì„œë„ ì•ˆì •ì **

---

## ğŸ”¹ 03. Attention ë©”ì»¤ë‹ˆì¦˜ì´ Seq2Seq ëª¨ë¸ì˜ ì–´ë–¤ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë° ë„ì›€ì´ ë˜ë‚˜ìš”?

### ğŸ“Œ Seq2Seq ëª¨ë¸ì˜ êµ¬ì¡°ì™€ í•œê³„

ê¸°ë³¸ì ì¸ **Sequence-to-Sequence(Seq2Seq)** ëª¨ë¸ì€

* **ì¸ì½”ë”(Encoder)**: ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ í•˜ë‚˜ì˜ ê³ ì • ë²¡í„°ë¡œ ì¸ì½”ë”©
* **ë””ì½”ë”(Decoder)**: í•´ë‹¹ ë²¡í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¶œë ¥ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±
  í•˜ëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤.

í•˜ì§€ë§Œ ì´ ë°©ì‹ì—ëŠ” ê·¼ë³¸ì ì¸ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤:

> ê¸´ ë¬¸ì¥ì˜ ì •ë³´ë¥¼ **í•˜ë‚˜ì˜ ê³ ì • ë²¡í„°**ì— ì••ì¶•í•˜ë¯€ë¡œ
> **ì •ë³´ ì†ì‹¤**ê³¼ **ë¬¸ë§¥ íë¦„ ë‹¨ì ˆ**ì´ ë°œìƒí•©ë‹ˆë‹¤.

### ğŸ“Œ Attention ë©”ì»¤ë‹ˆì¦˜ì˜ í•µì‹¬ ì•„ì´ë””ì–´

**Attention**ì€ â€œëª¨ë“  ë‹¨ì–´ë¥¼ ë™ì¼í•˜ê²Œ ë‹¤ë£¨ì§€ ë§ê³ , **í˜„ì¬ ì¶œë ¥ ë‹¨ì–´ë¥¼ ìƒì„±í•  ë•Œ ì¤‘ìš”í•œ ì…ë ¥ ë‹¨ì–´ì— ë” ì§‘ì¤‘í•˜ì**â€ëŠ” ê°œë…ì…ë‹ˆë‹¤.

ì¦‰, ë””ì½”ë”ê°€ ë§¤ ì‹œì ë§ˆë‹¤
ì…ë ¥ ë¬¸ì¥ì˜ ëª¨ë“  ë‹¨ì–´(hidden state)ì— ê°€ì¤‘ì¹˜(attention score)ë¥¼ ë¶€ì—¬í•˜ì—¬,
**ê°€ì¥ ê´€ë ¨ ìˆëŠ” ì •ë³´ì— â€˜ì§‘ì¤‘(attend)â€™**í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.

| í•­ëª©        | ì„¤ëª…                                   |
| :-------- | :----------------------------------- |
| **í•µì‹¬ ê°œë…** | ë””ì½”ë”ê°€ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ê° ë‹¨ì–´ì— ì¤‘ìš”ë„(ê°€ì¤‘ì¹˜)ë¥¼ ë™ì ìœ¼ë¡œ ê³„ì‚° |
| **íš¨ê³¼**    | ë¬¸ë§¥ì´ ê¸´ ë¬¸ì¥ì—ì„œë„ ê´€ë ¨ ë‹¨ì–´ ì •ë³´ë¥¼ ì„ íƒì ìœ¼ë¡œ ë°˜ì˜       |
| **ê²°ê³¼**    | ë” ìì—°ìŠ¤ëŸ½ê³  ë¬¸ë§¥ ì¼ê´€ì„± ìˆëŠ” ë²ˆì—­/ìƒì„± ê°€ëŠ¥           |

> **ë¹„ìœ ** ğŸ¯
> ì‚¬ëŒë„ ë¬¸ì¥ì„ ì´í•´í•  ë•Œ ëª¨ë“  ë‹¨ì–´ë¥¼ ë™ì¼í•˜ê²Œ ë³´ì§€ ì•Šê³ ,
> â€œí•µì‹¬ ë‹¨ì–´â€ì— ì§‘ì¤‘í•©ë‹ˆë‹¤.
> Attentionì€ ì´ëŸ° **ì¸ê°„ì˜ ì¸ì§€ ë°©ì‹**ì„ ìˆ˜í•™ì ìœ¼ë¡œ ëª¨ë¸ë§í•œ ê²ƒì…ë‹ˆë‹¤.

---

## ğŸ”¹ 04. Transformer ëª¨ë¸ì€ Seq2Seq êµ¬ì¡°ì™€ ì–´ë–¤ ì ì—ì„œ ê·¼ë³¸ì ìœ¼ë¡œ ë‹¤ë¥¸ê°€ìš”?

### ğŸ“Œ ê¸°ì¡´ Seq2Seq + Attentionì˜ í•œê³„

RNN ê¸°ë°˜ Seq2Seq ëª¨ë¸ì€ Attentionì„ ë„ì…í•´ë„ ì—¬ì „íˆ **ìˆœì°¨ì (Sequential)** êµ¬ì¡°ë¥¼ ë”°ë¦…ë‹ˆë‹¤.
ì¦‰, ì…ë ¥ì„ ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬í•´ì•¼ í•˜ë¯€ë¡œ **ë³‘ë ¬ ì²˜ë¦¬ ë¶ˆê°€**, **ê¸´ ë¬¸ì¥ í•™ìŠµ í•œê³„**, **ì‹œê°„ ì†Œëª¨** ë¬¸ì œê°€ ì¡´ì¬í–ˆìŠµë‹ˆë‹¤.

### ğŸ“Œ Transformerì˜ í˜ì‹ ì  ë³€í™”

**Transformer (Vaswani et al., 2017)**ëŠ” RNN êµ¬ì¡°ë¥¼ ì™„ì „íˆ ì œê±°í•˜ê³ ,
**Attentionë§Œìœ¼ë¡œ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•˜ëŠ” ëª¨ë¸**ì…ë‹ˆë‹¤.
ì´ë¥¼ í†µí•´ **ë³‘ë ¬ ì—°ì‚°**, **ì¥ê¸° ì˜ì¡´ì„±(Long Dependency) ì²˜ë¦¬**, **ì†ë„ í–¥ìƒ**ì´ ë™ì‹œì— ê°€ëŠ¥í•´ì¡ŒìŠµë‹ˆë‹¤.

| ë¹„êµ í•­ëª©                           | **ê¸°ì¡´ Seq2Seq (RNN ê¸°ë°˜)** | **Transformer**                           |
| :------------------------------ | :---------------------- | :---------------------------------------- |
| **ê¸°ë°˜ êµ¬ì¡°**                       | RNN/LSTM/GRU (ìˆœì°¨ ì²˜ë¦¬)    | Self-Attention (ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥)                 |
| **ë¬¸ì¥ ë‚´ ì˜ì¡´ì„± ì²˜ë¦¬**                 | ë‹¨ê¸° ì˜ì¡´ì„± ì¤‘ì‹¬               | ì¥ê¸° ì˜ì¡´ì„±ê¹Œì§€ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµ                         |
| **ë³‘ë ¬ ì—°ì‚°**                       | ë¶ˆê°€ëŠ¥                     | ê°€ëŠ¥ (GPU íš¨ìœ¨ì  í™œìš©)                           |
| **ëŒ€í‘œ êµ¬ì„± ìš”ì†Œ**                    | ì¸ì½”ë” + ë””ì½”ë” (RNN êµ¬ì¡°)      | ì¸ì½”ë” + ë””ì½”ë” (Self-Attention + Feed Forward) |
| **í¬ì§€ì…”ë„ ì¸ì½”ë”©(Position Encoding)** | í•„ìš” ì—†ìŒ (ìˆœì„œ ë‚´ì¥)           | í•„ìˆ˜ (ìˆœì„œ ì •ë³´ë¥¼ ë³´ì™„)                            |

> **í•µì‹¬ ìš”ì•½** âš™ï¸
>
> * TransformerëŠ” **â€œAttention is All You Needâ€**ë¼ëŠ” ì² í•™ ì•„ë˜
>   RNNì„ ì™„ì „íˆ ëŒ€ì²´í–ˆìŠµë‹ˆë‹¤.
> * ë•ë¶„ì— í˜„ëŒ€ì˜ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(GPT, BERT ë“±)ì˜ ê¸°ë°˜ì´ ë˜ì—ˆìŠµë‹ˆë‹¤.

---

> **ì •ë¦¬ ìš”ì•½** ğŸ§©
>
> * **í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬**: í…ìŠ¤íŠ¸ë¥¼ ì •ì œ â†’ í† í°í™” â†’ ìˆ˜ì¹˜í™” â†’ íŒ¨ë”©
> * **FastText**: Word2Vecì˜ í•œê³„ë¥¼ ê·¹ë³µí•œ **ì„œë¸Œì›Œë“œ ê¸°ë°˜ ì„ë² ë”©**
> * **Attention**: Seq2Seqì˜ **ì •ë³´ ì••ì¶• ë¬¸ì œ í•´ê²°**, ë¬¸ë§¥ ì„ íƒ ê°€ëŠ¥
> * **Transformer**: RNN ì œê±° + **Self-Attention ê¸°ë°˜ ë³‘ë ¬ êµ¬ì¡°**ë¡œ í˜ì‹ 

---

**ì¶œì²˜ ë° ì°¸ê³ **

* Vaswani et al., *Attention is All You Need* (2017)
* Mikolov et al., *Word2Vec: Efficient Estimation of Word Representations in Vector Space* (2013)
* Bojanowski et al., *Enriching Word Vectors with Subword Information (FastText)* (2016)
* ê¹€ê¸°í˜„, ã€Šë”¥ëŸ¬ë‹ ìì—°ì–´ì²˜ë¦¬ ì™„ë²½ ê°€ì´ë“œã€‹ (2023)
