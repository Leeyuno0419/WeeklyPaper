# 📘 [AI] 위클리페이퍼 \#12
---
> **작성일**: 2025년 10월 27일
>
> **작성자**: 이유노

## 🔹 01. LLM이 생성한 텍스트에서 할루시네이션(Hallucination)이란 무엇이고, 왜 문제가 되나요? 여러 LLM 서비스들은 할루시네이션 문제를 어떻게 극복하려고 시도 중일까요?

### 📌 LLM의 환각 현상 (Hallucination)

**할루시네이션(Hallucination)**, 즉 환각 현상은 대규모 언어 모델(LLM)이 **사실에 근거하지 않거나, 입력된 맥락과 관련 없는 정보를 그럴듯하게 생성하는 현상**을 말합니다. 모델이 학습한 데이터에 존재하지 않는 내용을 '지어내거나' 사실을 왜곡하면서도, 정답인 것처럼 확신에 찬 어조로 답변하는 것이 특징입니다.

### 📌 할루시네이션이 심각한 문제인 이유

| 문제점 | 설명 |
| :--- | :--- |
| **정보의 신뢰도 하락** | 사용자가 AI가 생성한 거짓 정보를 사실로 받아들여 잘못된 결정이나 오해를 유발할 수 있습니다. |
| **위험성 초래** | 의료, 법률, 금융 등 전문 분야에서 잘못된 정보를 제공할 경우 치명적인 결과를  B수 있습니다. |
| **기술 불신 확산** | 사용자가 AI 서비스의 답변을 신뢰할 수 없게 되면, 기술 자체에 대한 불신으로 이어질 수 있습니다. |

### 📌 주요 극복 방안 (Mitigation Strategies) 🔍

LLM 서비스 제공자들은 이 문제를 해결하기 위해 여러 기술을 복합적으로 사용하고 있습니다.

| 기법 | 설명 |
| :--- | :--- |
| **검색 증강 생성 (RAG)** | **(Retrieval-Augmented Generation)** 모델이 내부 지식에만 의존하지 않고, 답변 생성 전 **실시간으로 외부 데이터베이스나 웹(예: 구글 검색)에서 관련 정보를 검색**합니다. 그리고 이 검색된 '사실'에 기반(Grounding)하여 답변을 생성하도록 유도합니다. |
| **사실 일관성 체크** | 모델이 생성한 답변(후보)을 다시 입력하여, 원본 질문이나 검색된 자료와 **사실적 일관성이 맞는지 스스로 검증**하게 합니다. (예: "방금 네가 한 말이 이 자료와 일치하니?") |
| **출처 명시 (Citation)** | Google의 SGE(Search Generative Experience)처럼, **답변의 근거가 된 웹페이지나 문서를 함께 제공**하여 사용자가 직접 사실을 검증(Fact-checking)할 수 있도록 합니다. |
| **정제된 학습 데이터** | 할루시네이션을 유발할 수 있는 부정확하거나 편향된 데이터를 학습 과정에서 제거하고, 고품질의 검증된 데이터 위주로 학습시킵니다. |
| **RLHF 및 RLAIF** | 인간 피드백(RLHF)이나 AI 피드백(RLAIF)을 통해 모델이 사실에 입각한 답변을 생성할 때 더 높은 보상을 받도록 강화 학습을 진행합니다. |

-----

## 🔹 02. 모델 크기를 키우는 것만으로는 성능이 일정 시점 이후 둔화되는 이유는 무엇일까요?

### 📌 모델 스케일링의 한계 (Scaling Law's Limits)

과거에는 모델의 파라미터(매개변수) 크기를 키우는 것(Scaling up)이 곧 성능 향상의 지름길로 여겨졌습니다. 하지만 최근 연구(예: DeepMind의 Chinchilla)에 따르면, 모델 크기만 무작정 키우는 것은 일정 수준 이상에서는 비효율적이며 성능 향상이 둔화됩니다.

**핵심적인 이유는 모델의 크기와 학습 데이터의 양이 균형 있게 증가하지 않았기 때문입니다.**

> **핵심 아이디어** 📈
>
> "모델이 너무 똑똑해져도, 배울 교과서(데이터)가 부실하면 더 이상 성장할 수 없다."
>
> 모델의 성능은 **모델의 크기(Parameters)**뿐만 아니라 **학습 데이터의 품질과 양(Tokens)**, 그리고 **컴퓨팅 자원**이라는 세 가지 요소에 의해 복합적으로 결정됩니다.

### 📌 성능 둔화의 주요 원인

| 원인 | 설명 |
| :--- | :--- |
| 1. **데이터 품질 및 수량의 한계** | 모델이 학습 데이터에 포함된 지식과 패턴을 거의 다 학습하고 나면, 모델 크기를 더 키워도 **데이터 자체가 담고 있는 정보의 총량을 넘어설 수 없습니다.** (데이터 병목 현상) |
| 2. **수확 체감의 법칙** | 모델 크기를 2배로 늘려도 성능은 2배로 증가하지 않습니다. 파라미터를 늘릴수록 **성능 향상 폭은 점차 줄어드는 반면, 학습 및 추론에 필요한 비용(시간, 전력)은 기하급수적으로 증가**하여 비효율적이 됩니다. |
| 3. **과적합(Overfitting) 위험** | 데이터의 양에 비해 모델이 과도하게 클 경우, 모델이 데이터의 일반적인 패턴이 아닌 **노이즈(Noise)나 특정 사례까지 통째로 암기**하려 해 오히려 새로운 데이터에 대한 일반화 성능이 떨어질 수 있습니다. |

-----

## 🔹 03. PEFT가 필요한 이유는 무엇이며, 어떤 상황에서 특히 효과적인가요?

### 📌 PEFT (Parameter-Efficient Fine-Tuning)란?

**PEFT(패프트)**는 **'파라미터 효율적 미세 조정'**을 의미합니다. 수천억 개에 달하는 거대 언어 모델(LLM)의 **모든 파라미터를 업데이트하지 않고, 아주 일부의 파라미터만(혹은 추가된 작은 모듈만) 학습**시켜 특정 작업(Task)에 적응시키는 기술입니다.

기존 방식(Full Fine-Tuning)이 모델 전체를 복사해 통째로 재학습시켰다면, PEFT는 원본 모델(Pre-trained Model)은 **'동결(Freeze)'**시킨 채, '어댑터(Adapter)'나 'LoRA' 같은 작은 부품만 추가하여 학습시킵니다.

### 📌 PEFT가 필요한 이유: 비용 절감 💡

| 비교 | **전체 미세 조정 (Full Fine-Tuning)** | **PEFT (예: LoRA)** |
| :--- | :--- | :--- |
| **학습 대상** | 모델의 모든 파라미터 (예: 1,750억 개) | 모델의 극히 일부 (예: 0.1% = 1.7억 개) |
| **필요 자원** | 막대한 양의 VRAM (최고 사양 GPU 수십 장) | 상대적으로 적은 VRAM (단일 GPU로도 가능) |
| **학습 시간** | 수일 ~ 수 주 | 수 시간 ~ 수 일 |
| **결과물 (저장)** | **모델 전체 사본** (수백 GB) | **작은 어댑터 가중치** (수 MB ~ 수백 MB) |
| **치명적 단점** | 100개 작업을 학습시키면 **모델 100개** 저장 필요 | 100개 작업을 학습해도 **원본 1개 + 어댑터 100개** |

### 📌 PEFT가 특히 효과적인 상황

1.  **제한된 컴퓨팅 자원 (예산/GPU 부족)**
    * Full Fine-Tuning에 필요한 고성능 GPU(A100, H100 등)가 부족하거나 없을 때, PEFT는 훨씬 적은 VRAM으로도 모델 튜닝을 가능하게 합니다.

2.  **다중 작업(Multi-Task) 처리 필요**
    * 하나의 거대 원본 모델을 기반으로 '요약', '번역', '질의응답', '감성 분석' 등 **수십, 수백 개의 서로 다른 작업을 수행하는 모델**을 만들어야 할 때 매우 효율적입니다. 모델 전체를 복제할 필요 없이, 작업별로 용량이 작은 PEFT 가중치만 저장하고 교체(Swap)하면 됩니다.

3.  **치명적 망각(Catastrophic Forgetting) 방지**
    * Full Fine-Tuning 시, 모델이 새로운 작업에 과도하게 적응하느라 기존에 학습했던 일반 지식(예: "하늘은 파랗다")을 잊어버리는 '치명적 망각' 문제가 발생할 수 있습니다. PEFT는 원본 모델을 동결시키므로, **기존의 방대한 지식을 보존**하면서 새로운 작업 수행 능력을 '추가'하는 데 유리합니다.
